{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ü§ñ Churnito - Sistema de Predicci√≥n de Churn (Local)\n",
    "\n",
    "**Sistema inteligente para predecir y prevenir la fuga de clientes usando Machine Learning y LLM**\n",
    "\n",
    "**Versi√≥n optimizada para ejecuci√≥n local** (con GPU si est√° disponible)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Contenido:\n",
    "1. Requisitos previos e instalaci√≥n\n",
    "2. Carga de datos\n",
    "3. Entrenamiento del modelo de predicci√≥n\n",
    "4. Descarga del LLM (Qwen2.5)\n",
    "5. Sistema de chat interactivo con Churnito\n",
    "6. An√°lisis y visualizaciones\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Requisitos Previos\n",
    "\n",
    "### Instalaci√≥n de dependencias (ejecutar en terminal):\n",
    "\n",
    "```bash\n",
    "# Opci√≥n 1: Con pip\n",
    "pip install torch torchvision torchaudio\n",
    "pip install transformers>=4.30.0\n",
    "pip install accelerate>=0.26.0\n",
    "pip install datasets>=2.14.0\n",
    "pip install scikit-learn>=1.3.0\n",
    "pip install pandas numpy\n",
    "\n",
    "# Opci√≥n 2: Con conda (recomendado)\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "conda install -c huggingface transformers\n",
    "pip install accelerate datasets scikit-learn pandas numpy\n",
    "```\n",
    "\n",
    "### Requisitos de hardware:\n",
    "- **CPU:** Funcional pero lento (~10-15 min entrenamiento)\n",
    "- **GPU (recomendado):** NVIDIA con CUDA (~2-3 min entrenamiento)\n",
    "- **RAM:** M√≠nimo 8GB, recomendado 16GB\n",
    "- **Disco:** ~10GB para modelos y cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Importar Bibliotecas y Verificar GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Verificar GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas\")\n",
    "print(f\"üñ•Ô∏è  PyTorch versi√≥n: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  Dispositivo: {device}\")\n",
    "print(f\"üñ•Ô∏è  CUDA disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üéÆ Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No se detect√≥ GPU. El entrenamiento ser√° en CPU (m√°s lento)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Cargar Dataset\n",
    "\n",
    "El notebook buscar√° primero el archivo localmente, si no lo encuentra lo descargar√°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Buscar archivo local primero\n",
    "filename = \"Churn_Modelling.csv\"\n",
    "local_paths = [\n",
    "    filename,\n",
    "    f\"../{filename}\",\n",
    "    f\"data/{filename}\"\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for path in local_paths:\n",
    "    if os.path.exists(path):\n",
    "        dataset_path = path\n",
    "        print(f\"‚úÖ Dataset encontrado en: {path}\")\n",
    "        break\n",
    "\n",
    "# Si no existe, descargar desde GitHub\n",
    "if dataset_path is None:\n",
    "    url = \"https://raw.githubusercontent.com/CuchoLeo/Fuga/main/Churn_Modelling.csv\"\n",
    "    print(\"üì• Descargando dataset desde GitHub...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    dataset_path = filename\n",
    "    print(\"‚úÖ Dataset descargado\")\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"\\nüìä Dataset cargado: {len(df)} registros, {len(df.columns)} columnas\")\n",
    "print(f\"\\nColumnas: {list(df.columns)}\")\n",
    "print(f\"\\nüìà Distribuci√≥n de Churn:\")\n",
    "print(df['Exited'].value_counts())\n",
    "print(f\"\\nüéØ Tasa de Churn: {df['Exited'].mean()*100:.2f}%\")\n",
    "\n",
    "# Vista previa\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üßπ Preprocesando datos...\")\n",
    "\n",
    "# Eliminar columnas no relevantes\n",
    "cols_to_drop = ['RowNumber', 'CustomerId', 'Surname']\n",
    "df_clean = df.drop([col for col in cols_to_drop if col in df.columns], axis=1)\n",
    "\n",
    "# Codificar variables categ√≥ricas\n",
    "label_encoders = {}\n",
    "categorical_cols = ['Geography', 'Gender']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_clean.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_clean[col] = le.fit_transform(df_clean[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Separar features y target\n",
    "X = df_clean.drop('Exited', axis=1)\n",
    "y = df_clean['Exited']\n",
    "\n",
    "# Normalizar features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Dividir en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y.values, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocesamiento completado\")\n",
    "print(f\"üìä Train: {len(X_train)} muestras\")\n",
    "print(f\"üìä Test: {len(X_test)} muestras\")\n",
    "print(f\"üìä Features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Crear Dataset para PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para predicci√≥n de churn\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, feature_names, tokenizer, max_length=256):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature_names = feature_names\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Crear descripci√≥n textual del cliente\n",
    "        features = self.X[idx]\n",
    "        text_parts = [\"Cliente:\"]\n",
    "        \n",
    "        for name, value in zip(self.feature_names, features):\n",
    "            text_parts.append(f\"{name}={value:.2f}\")\n",
    "        \n",
    "        text = \" \".join(text_parts)\n",
    "        \n",
    "        # Tokenizar\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Clase ChurnDataset definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Entrenar Modelo de Predicci√≥n de Churn\n",
    "\n",
    "Usamos DistilBERT para clasificaci√≥n binaria (churn/no-churn).\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado:** \n",
    "- CPU: ~10-15 minutos\n",
    "- GPU: ~2-3 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üöÄ ENTRENANDO MODELO DE PREDICCI√ìN DE CHURN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar modelo base\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "print(f\"\\nüì¶ Cargando {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modelo base cargado\")\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = ChurnDataset(X_train, y_train, feature_names, tokenizer)\n",
    "test_dataset = ChurnDataset(X_test, y_test, feature_names, tokenizer)\n",
    "\n",
    "# ============================================================================\n",
    "# CALCULAR CLASS WEIGHTS PARA MANEJAR DESBALANCE\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weights_array, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  Balanceo de clases:\")\n",
    "print(f\"Clase 0 (NO CHURN): {(y_train == 0).sum()} muestras, weight={class_weights[0]:.3f}\")\n",
    "print(f\"Clase 1 (CHURN):    {(y_train == 1).sum()} muestras, weight={class_weights[1]:.3f}\")\n",
    "print(f\"Ratio: {class_weights[1]/class_weights[0]:.2f}x m√°s peso para clase minoritaria\")\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM TRAINER CON CLASS WEIGHTS (versi√≥n compatible con GPU/CPU)\n",
    "# ============================================================================\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Trainer personalizado que usa class weights en la funci√≥n de p√©rdida\"\"\"\n",
    "\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Extraer labels (sin modificar inputs)\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Calcular p√©rdida con class weights\n",
    "        if self.class_weights is not None:\n",
    "            # IMPORTANTE: Mover class_weights al mismo dispositivo que el modelo\n",
    "            device = logits.device\n",
    "            class_weights_device = self.class_weights.to(device)\n",
    "            \n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_device)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            # Fallback a p√©rdida est√°ndar\n",
    "            loss = outputs.get(\"loss\")\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Configuraci√≥n de entrenamiento (optimizada para local)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./churn_checkpoint\",\n",
    "    num_train_epochs=3,  # 3 √©pocas para mejor rendimiento\n",
    "    per_device_train_batch_size=32 if torch.cuda.is_available() else 16,  # Ajustar seg√∫n GPU\n",
    "    per_device_eval_batch_size=32 if torch.cuda.is_available() else 16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),  # Activar mixed precision en GPU\n",
    ")\n",
    "\n",
    "# Funci√≥n de m√©tricas\n",
    "def compute_metrics(eval_pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    try:\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        print(f\"\\nüìä Matriz de Confusi√≥n:\")\n",
    "        print(f\"   TN={tn}, FP={fp}\")\n",
    "        print(f\"   FN={fn}, TP={tp}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Crear Trainer con class weights\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,  # Pasar class weights\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "print(\"\\nüèãÔ∏è Iniciando entrenamiento...\")\n",
    "print(f\"üñ•Ô∏è  Dispositivo: {device}\")\n",
    "print(f\"üñ•Ô∏è  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"üñ•Ô∏è  √âpocas: {training_args.num_train_epochs}\")\n",
    "print(f\"üñ•Ô∏è  FP16 (mixed precision): {training_args.fp16}\\n\")\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Evaluar\n",
    "    print(\"\\nüìä Evaluando modelo...\\n\")\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ ENTRENAMIENTO COMPLETADO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìä Accuracy:  {eval_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"üìä Precision: {eval_results['eval_precision']:.4f}\")\n",
    "    print(f\"üìä Recall:    {eval_results['eval_recall']:.4f}\")\n",
    "    print(f\"üìä F1-Score:  {eval_results['eval_f1']:.4f}\")\n",
    "\n",
    "    # Interpretaci√≥n\n",
    "    print(\"\\nüí° Interpretaci√≥n:\")\n",
    "    print(\"   - Accuracy:  % de predicciones correctas (total)\")\n",
    "    print(\"   - Precision: De los que predecimos CHURN, % que realmente hacen churn\")\n",
    "    print(\"   - Recall:    De los que hacen CHURN, % que detectamos correctamente\")\n",
    "    print(\"   - F1-Score:  Balance entre Precision y Recall\")\n",
    "\n",
    "    # Advertencias y recomendaciones\n",
    "    if eval_results['eval_precision'] < 0.5:\n",
    "        print(\"\\n‚ö†Ô∏è  ADVERTENCIA: Precision baja. El modelo predice muchos falsos positivos.\")\n",
    "    if eval_results['eval_recall'] < 0.5:\n",
    "        print(\"\\n‚ö†Ô∏è  ADVERTENCIA: Recall bajo. El modelo no detecta suficientes churners.\")\n",
    "    if eval_results['eval_f1'] > 0.7:\n",
    "        print(\"\\n‚úÖ F1-Score bueno (>0.7). Modelo balanceado entre precision y recall.\")\n",
    "\n",
    "    # Guardar modelo\n",
    "    model.save_pretrained(\"./churn_model\")\n",
    "    tokenizer.save_pretrained(\"./churn_model\")\n",
    "\n",
    "    # Guardar artefactos de preprocesamiento\n",
    "    with open(\"./churn_model/preprocessing_artifacts.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            'scaler': scaler,\n",
    "            'label_encoders': label_encoders,\n",
    "            'feature_names': feature_names\n",
    "        }, f)\n",
    "\n",
    "    print(\"\\nüíæ Modelo guardado en: ./churn_model/\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error durante entrenamiento: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Descargar LLM para Chat (Qwen2.5)\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado:** 2-5 minutos (descarga ~3GB)\n",
    "\n",
    "**Nota:** El LLM se cargar√° en GPU autom√°ticamente si est√° disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Descargando LLM Qwen2.5-1.5B-Instruct...\")\n",
    "print(\"üì• Esto puede tardar 2-5 minutos (~3GB)\\n\")\n",
    "\n",
    "llm_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llm_model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Cargar en GPU si est√° disponible\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_id,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    llm_model = llm_model.to(device)\n",
    "\n",
    "print(\"\\n‚úÖ LLM cargado exitosamente\")\n",
    "print(f\"üñ•Ô∏è  Dispositivo LLM: {device}\")\n",
    "print(\"üí¨ Churnito est√° listo para conversar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Sistema Churnito - Clase Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnitoSystem:\n",
    "    \"\"\"Sistema completo de an√°lisis de churn con chat conversacional\"\"\"\n",
    "    \n",
    "    def __init__(self, churn_model_path=\"./churn_model\"):\n",
    "        print(\"üîÑ Inicializando Churnito...\")\n",
    "        \n",
    "        # Cargar modelo de churn\n",
    "        self.churn_tokenizer = AutoTokenizer.from_pretrained(churn_model_path)\n",
    "        self.churn_model = AutoModelForSequenceClassification.from_pretrained(churn_model_path)\n",
    "        self.churn_model.eval()\n",
    "        \n",
    "        # Mover a GPU si est√° disponible\n",
    "        self.churn_model = self.churn_model.to(device)\n",
    "        \n",
    "        # Cargar artefactos\n",
    "        with open(f\"{churn_model_path}/preprocessing_artifacts.pkl\", \"rb\") as f:\n",
    "            artifacts = pickle.load(f)\n",
    "            self.scaler = artifacts['scaler']\n",
    "            self.label_encoders = artifacts['label_encoders']\n",
    "            self.feature_names = artifacts['feature_names']\n",
    "        \n",
    "        # LLM (ya cargado globalmente)\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "        self.llm_model = llm_model\n",
    "        \n",
    "        # Base de datos de clientes\n",
    "        self.customer_database = df\n",
    "        \n",
    "        print(f\"‚úÖ Churnito inicializado en {device}\")\n",
    "    \n",
    "    def predict_single_customer(self, customer_data):\n",
    "        \"\"\"Predice churn para un cliente\"\"\"\n",
    "        # Codificar categ√≥ricas\n",
    "        processed = customer_data.copy()\n",
    "        for col_name, encoder in self.label_encoders.items():\n",
    "            if col_name in processed:\n",
    "                try:\n",
    "                    processed[col_name] = encoder.transform([processed[col_name]])[0]\n",
    "                except:\n",
    "                    processed[col_name] = 0\n",
    "        \n",
    "        # Preparar features\n",
    "        features = [float(processed.get(f, 0)) for f in self.feature_names]\n",
    "        features_scaled = self.scaler.transform([features])\n",
    "        \n",
    "        # Crear texto\n",
    "        text = \"Cliente: \" + \" \".join([f\"{n}={v:.2f}\" for n, v in zip(self.feature_names, features_scaled[0])])\n",
    "        \n",
    "        # Predecir\n",
    "        inputs = self.churn_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.churn_model(**inputs)\n",
    "            probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "            churn_prob = probabilities[0][1].item()\n",
    "        \n",
    "        return churn_prob\n",
    "    \n",
    "    def get_at_risk_customers(self, limit=10):\n",
    "        \"\"\"Obtiene clientes en riesgo\"\"\"\n",
    "        df_sample = self.customer_database.sample(n=min(100, len(self.customer_database)), random_state=42)\n",
    "        \n",
    "        at_risk = []\n",
    "        for idx, row in df_sample.iterrows():\n",
    "            customer_data = {\n",
    "                'CreditScore': row.get('CreditScore', 0),\n",
    "                'Geography': row.get('Geography', ''),\n",
    "                'Gender': row.get('Gender', ''),\n",
    "                'Age': row.get('Age', 0),\n",
    "                'Tenure': row.get('Tenure', 0),\n",
    "                'Balance': row.get('Balance', 0),\n",
    "                'NumOfProducts': row.get('NumOfProducts', 0),\n",
    "                'HasCrCard': row.get('HasCrCard', 0),\n",
    "                'IsActiveMember': row.get('IsActiveMember', 0),\n",
    "                'EstimatedSalary': row.get('EstimatedSalary', 0)\n",
    "            }\n",
    "            \n",
    "            churn_prob = self.predict_single_customer(customer_data)\n",
    "            \n",
    "            if churn_prob > 0.5:\n",
    "                at_risk.append({\n",
    "                    'customer_id': int(row.get('CustomerId', idx)),\n",
    "                    'churn_probability': churn_prob,\n",
    "                    'balance': customer_data['Balance'],\n",
    "                    'age': customer_data['Age'],\n",
    "                    'is_active': bool(customer_data['IsActiveMember'])\n",
    "                })\n",
    "        \n",
    "        at_risk.sort(key=lambda x: x['churn_probability'], reverse=True)\n",
    "        return at_risk[:limit]\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Obtiene estad√≠sticas del dataset\"\"\"\n",
    "        return {\n",
    "            'total_customers': len(self.customer_database),\n",
    "            'churned_customers': int(self.customer_database['Exited'].sum()),\n",
    "            'churn_rate': float(self.customer_database['Exited'].mean()),\n",
    "            'avg_balance': float(self.customer_database['Balance'].mean()),\n",
    "            'avg_age': float(self.customer_database['Age'].mean())\n",
    "        }\n",
    "    \n",
    "    def generate_structured_response(self, query, context):\n",
    "        \"\"\"Genera respuesta estructurada con datos reales\"\"\"\n",
    "        response = []\n",
    "        \n",
    "        if \"at_risk_customers\" in context:\n",
    "            at_risk = context[\"at_risk_customers\"]\n",
    "            if at_risk:\n",
    "                high_value_count = sum(1 for c in at_risk if c['balance'] > 100000)\n",
    "                inactive_count = sum(1 for c in at_risk if not c['is_active'])\n",
    "                \n",
    "                response.append(f\"üéØ **An√°lisis de Clientes en Riesgo:**\")\n",
    "                response.append(f\"   ‚Ä¢ {len(at_risk)} clientes identificados con alta probabilidad de churn\")\n",
    "                response.append(f\"   ‚Ä¢ {high_value_count} son clientes de alto valor (Balance > $100k)\")\n",
    "                response.append(f\"   ‚Ä¢ {inactive_count} clientes est√°n inactivos\")\n",
    "                \n",
    "                response.append(\"\\nüìä **Top 5 Clientes Prioritarios:**\")\n",
    "                for i, customer in enumerate(at_risk[:5], 1):\n",
    "                    prob_pct = customer['churn_probability'] * 100\n",
    "                    response.append(\n",
    "                        f\"   {i}. Cliente #{customer['customer_id']}: {prob_pct:.1f}% riesgo, \"\n",
    "                        f\"${customer['balance']:,.0f} balance\\n\"\n",
    "                        f\"      ‚Üí {'üî¥ INACTIVO' if not customer['is_active'] else 'üü° Activo'}\"\n",
    "                    )\n",
    "        \n",
    "        elif \"statistics\" in context:\n",
    "            stats = context[\"statistics\"]\n",
    "            churn_rate = stats['churn_rate'] * 100\n",
    "            \n",
    "            response.append(\"üìä **Estad√≠sticas Actuales:**\")\n",
    "            response.append(f\"   ‚Ä¢ Total de clientes: {stats['total_customers']:,}\")\n",
    "            response.append(f\"   ‚Ä¢ Tasa de churn: {churn_rate:.2f}%\")\n",
    "            response.append(f\"   ‚Ä¢ Balance promedio: ${stats['avg_balance']:,.2f}\")\n",
    "            response.append(f\"   ‚Ä¢ Edad promedio: {stats['avg_age']:.1f} a√±os\")\n",
    "        \n",
    "        return \"\\n\".join(response) if response else \"Churnito a tu servicio. ¬øEn qu√© puedo ayudarte?\"\n",
    "    \n",
    "    def chat(self, query):\n",
    "        \"\"\"Procesa una consulta de chat\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        context = {}\n",
    "        \n",
    "        # Detectar intenciones\n",
    "        if any(word in query_lower for word in [\"riesgo\", \"top\", \"clientes\", \"fuga\", \"muestra\"]):\n",
    "            context[\"at_risk_customers\"] = self.get_at_risk_customers(limit=10)\n",
    "        \n",
    "        if any(word in query_lower for word in [\"estad√≠stica\", \"tasa\", \"cu√°ntos\", \"total\"]):\n",
    "            context[\"statistics\"] = self.get_statistics()\n",
    "        \n",
    "        # Generar respuesta\n",
    "        return self.generate_structured_response(query, context)\n",
    "\n",
    "# Inicializar Churnito\n",
    "churnito = ChurnitoSystem()\n",
    "print(\"\\nü§ñ ¬°Churnito est√° listo para conversar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Chat Interactivo con Churnito\n",
    "\n",
    "¬°Haz preguntas a Churnito sobre tus clientes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_churnito():\n",
    "    \"\"\"Interfaz de chat simple\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"üí¨ CHAT CON CHURNITO\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nEjemplos de preguntas:\")\n",
    "    print(\"  ‚Ä¢ Mu√©strame los 10 clientes con mayor riesgo de fuga\")\n",
    "    print(\"  ‚Ä¢ ¬øCu√°l es la tasa de churn actual?\")\n",
    "    print(\"  ‚Ä¢ Dame estad√≠sticas generales\")\n",
    "    print(\"\\nEscribe 'salir' para terminar\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nüßë T√∫: \")\n",
    "        \n",
    "        if query.lower() in ['salir', 'exit', 'quit']:\n",
    "            print(\"\\nüëã ¬°Hasta luego!\")\n",
    "            break\n",
    "        \n",
    "        if not query.strip():\n",
    "            continue\n",
    "        \n",
    "        response = churnito.chat(query)\n",
    "        print(f\"\\nü§ñ Churnito:\\n{response}\")\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Iniciar chat\n",
    "chat_with_churnito()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## üîü Consultas R√°pidas (sin interfaz interactiva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 1: Clientes en riesgo\n",
    "print(\"üìä CONSULTA: Clientes en riesgo\\n\")\n",
    "response = churnito.chat(\"Mu√©strame los 10 clientes con mayor riesgo de fuga\")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Ejemplo 2: Estad√≠sticas\n",
    "print(\"üìä CONSULTA: Estad√≠sticas generales\\n\")\n",
    "response = churnito.chat(\"Dame las estad√≠sticas de churn\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Predicci√≥n para Cliente Espec√≠fico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cliente\n",
    "ejemplo_cliente = {\n",
    "    'CreditScore': 650,\n",
    "    'Geography': 'France',\n",
    "    'Gender': 'Female',\n",
    "    'Age': 42,\n",
    "    'Tenure': 2,\n",
    "    'Balance': 125000,\n",
    "    'NumOfProducts': 1,\n",
    "    'HasCrCard': 1,\n",
    "    'IsActiveMember': 0,\n",
    "    'EstimatedSalary': 75000\n",
    "}\n",
    "\n",
    "churn_prob = churnito.predict_single_customer(ejemplo_cliente)\n",
    "\n",
    "print(\"üîç PREDICCI√ìN PARA CLIENTE ESPEC√çFICO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Datos del cliente:\")\n",
    "for key, value in ejemplo_cliente.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüéØ Probabilidad de churn: {churn_prob*100:.2f}%\")\n",
    "print(f\"üö¶ Nivel de riesgo: {'üî¥ ALTO' if churn_prob > 0.7 else 'üü° MEDIO' if churn_prob > 0.5 else 'üü¢ BAJO'}\")\n",
    "\n",
    "if churn_prob > 0.7:\n",
    "    print(\"\\n‚ö†Ô∏è  RECOMENDACI√ìN: Contactar inmediatamente para retenci√≥n\")\n",
    "elif churn_prob > 0.5:\n",
    "    print(\"\\nüí° RECOMENDACI√ìN: Implementar programa de retenci√≥n preventivo\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ RECOMENDACI√ìN: Monitoreo rutinario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Exportar Modelos (Opcional)\n",
    "\n",
    "Comprime el modelo entrenado para compartir o respaldar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Comprimir modelo de churn\n",
    "output_filename = \"churn_model\"\n",
    "shutil.make_archive(output_filename, 'zip', './churn_model')\n",
    "\n",
    "print(f\"‚úÖ Modelo comprimido: {output_filename}.zip\")\n",
    "print(f\"üì¶ Tama√±o: {os.path.getsize(f'{output_filename}.zip') / 1024**2:.2f} MB\")\n",
    "print(f\"üíæ Ubicaci√≥n: {os.path.abspath(f'{output_filename}.zip')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ ¬°Felicidades!\n",
    "\n",
    "Has completado la configuraci√≥n de Churnito en tu entorno local.\n",
    "\n",
    "### üí° Ventajas de la versi√≥n local:\n",
    "- ‚úÖ Entrenamiento m√°s r√°pido con GPU (si est√° disponible)\n",
    "- ‚úÖ 3 √©pocas para mejor rendimiento del modelo\n",
    "- ‚úÖ Mixed precision (FP16) para mayor velocidad\n",
    "- ‚úÖ Sin l√≠mites de tiempo de ejecuci√≥n\n",
    "- ‚úÖ Modelos guardados localmente para reutilizaci√≥n\n",
    "\n",
    "### üìö Recursos adicionales:\n",
    "- [Repositorio GitHub](https://github.com/CuchoLeo/Fuga)\n",
    "- [Documentaci√≥n completa](https://github.com/CuchoLeo/Fuga/blob/main/README.md)\n",
    "- [Versi√≥n Colab](https://colab.research.google.com/github/CuchoLeo/Fuga/blob/main/Churnito_Colab.ipynb)\n",
    "\n",
    "### ü§ù Creado por:\n",
    "**Churnito Team** - Sistema de predicci√≥n de churn con IA\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
