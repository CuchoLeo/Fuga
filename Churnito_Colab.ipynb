{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ü§ñ Churnito - Sistema de Predicci√≥n de Churn con IA\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CuchoLeo/Fuga/blob/main/Churnito_Colab.ipynb)\n",
    "\n",
    "**Sistema inteligente para predecir y prevenir la fuga de clientes usando Machine Learning y LLM**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Contenido:\n",
    "1. Instalaci√≥n de dependencias\n",
    "2. Carga de datos\n",
    "3. Entrenamiento del modelo de predicci√≥n\n",
    "4. Descarga del LLM (Qwen2.5)\n",
    "5. Sistema de chat interactivo con Churnito\n",
    "6. An√°lisis y visualizaciones\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1"
   },
   "source": [
    "## 1Ô∏è‚É£ Instalaci√≥n de Dependencias\n",
    "\n",
    "Instalamos todas las bibliotecas necesarias para el sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Instalaci√≥n silenciosa de dependencias\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install transformers>=4.30.0\n",
    "!pip install accelerate>=0.26.0\n",
    "!pip install datasets>=2.14.0\n",
    "!pip install scikit-learn>=1.3.0\n",
    "!pip install pandas numpy\n",
    "\n",
    "print(\"‚úÖ Todas las dependencias instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2"
   },
   "source": [
    "## 2Ô∏è‚É£ Importar Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas\")\n",
    "print(f\"üñ•Ô∏è  PyTorch versi√≥n: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section3"
   },
   "source": [
    "## 3Ô∏è‚É£ Cargar Dataset\n",
    "\n",
    "Puedes:\n",
    "- **Opci√≥n A:** Subir el archivo `Churn_Modelling.csv` usando el bot√≥n de archivos (üìÅ) en el panel izquierdo\n",
    "- **Opci√≥n B:** Descargarlo desde el repositorio de GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Opci√≥n A: Si subiste el archivo manualmente\n",
    "# df = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "# Opci√≥n B: Descargar desde GitHub\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CuchoLeo/Fuga/main/Churn_Modelling.csv\"\n",
    "filename = \"Churn_Modelling.csv\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    print(\"üì• Descargando dataset...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(\"‚úÖ Dataset descargado\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset ya existe\")\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "print(f\"\\nüìä Dataset cargado: {len(df)} registros, {len(df.columns)} columnas\")\n",
    "print(f\"\\nColumnas: {list(df.columns)}\")\n",
    "print(f\"\\nüìà Distribuci√≥n de Churn:\")\n",
    "print(df['Exited'].value_counts())\n",
    "print(f\"\\nüéØ Tasa de Churn: {df['Exited'].mean()*100:.2f}%\")\n",
    "\n",
    "# Vista previa\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4"
   },
   "source": [
    "## 4Ô∏è‚É£ Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess"
   },
   "outputs": [],
   "source": [
    "print(\"üßπ Preprocesando datos...\")\n",
    "\n",
    "# Eliminar columnas no relevantes\n",
    "cols_to_drop = ['RowNumber', 'CustomerId', 'Surname']\n",
    "df_clean = df.drop([col for col in cols_to_drop if col in df.columns], axis=1)\n",
    "\n",
    "# Codificar variables categ√≥ricas\n",
    "label_encoders = {}\n",
    "categorical_cols = ['Geography', 'Gender']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_clean.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_clean[col] = le.fit_transform(df_clean[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Separar features y target\n",
    "X = df_clean.drop('Exited', axis=1)\n",
    "y = df_clean['Exited']\n",
    "\n",
    "# Normalizar features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Dividir en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y.values, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocesamiento completado\")\n",
    "print(f\"üìä Train: {len(X_train)} muestras\")\n",
    "print(f\"üìä Test: {len(X_test)} muestras\")\n",
    "print(f\"üìä Features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section5"
   },
   "source": [
    "## 5Ô∏è‚É£ Crear Dataset para PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset"
   },
   "outputs": [],
   "source": [
    "class ChurnDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para predicci√≥n de churn\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, feature_names, tokenizer, max_length=256):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature_names = feature_names\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Crear descripci√≥n textual del cliente\n",
    "        features = self.X[idx]\n",
    "        text_parts = [\"Cliente:\"]\n",
    "        \n",
    "        for name, value in zip(self.feature_names, features):\n",
    "            text_parts.append(f\"{name}={value:.2f}\")\n",
    "        \n",
    "        text = \" \".join(text_parts)\n",
    "        \n",
    "        # Tokenizar\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Clase ChurnDataset definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section6"
   },
   "source": [
    "## 6Ô∏è‚É£ Entrenar Modelo de Predicci√≥n de Churn\n",
    "\n",
    "Usamos DistilBERT para clasificaci√≥n binaria (churn/no-churn).\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado:** 3-5 minutos en CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üöÄ ENTRENANDO MODELO DE PREDICCI√ìN DE CHURN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar modelo base\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "print(f\"\\nüì¶ Cargando {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modelo base cargado\")\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = ChurnDataset(X_train, y_train, feature_names, tokenizer)\n",
    "test_dataset = ChurnDataset(X_test, y_test, feature_names, tokenizer)\n",
    "\n",
    "# Configuraci√≥n de entrenamiento (optimizada para Colab)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./churn_checkpoint\",\n",
    "    num_train_epochs=1,  # 1 √©poca para rapidez\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",  # Desactivar reportes\n",
    ")\n",
    "\n",
    "# Funci√≥n de m√©tricas\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Crear Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "print(\"\\nüèãÔ∏è Iniciando entrenamiento...\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Evaluar\n",
    "print(\"\\nüìä Evaluando modelo...\\n\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ENTRENAMIENTO COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"üìä Precision: {eval_results['eval_precision']:.4f}\")\n",
    "print(f\"üìä Recall: {eval_results['eval_recall']:.4f}\")\n",
    "print(f\"üìä F1-Score: {eval_results['eval_f1']:.4f}\")\n",
    "\n",
    "# Guardar modelo\n",
    "model.save_pretrained(\"./churn_model\")\n",
    "tokenizer.save_pretrained(\"./churn_model\")\n",
    "\n",
    "# Guardar artefactos de preprocesamiento\n",
    "with open(\"./churn_model/preprocessing_artifacts.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        'scaler': scaler,\n",
    "        'label_encoders': label_encoders,\n",
    "        'feature_names': feature_names\n",
    "    }, f)\n",
    "\n",
    "print(\"\\nüíæ Modelo guardado en: ./churn_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section7"
   },
   "source": [
    "## 7Ô∏è‚É£ Descargar LLM para Chat (Qwen2.5)\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado:** 2-5 minutos (descarga ~3GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_llm"
   },
   "outputs": [],
   "source": [
    "print(\"ü§ñ Descargando LLM Qwen2.5-1.5B-Instruct...\")\n",
    "print(\"üì• Esto puede tardar 2-5 minutos (~3GB)\\n\")\n",
    "\n",
    "llm_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llm_model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ LLM cargado exitosamente\")\n",
    "print(\"üí¨ Churnito est√° listo para conversar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section8"
   },
   "source": [
    "## 8Ô∏è‚É£ Sistema Churnito - Clase Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "churnito_class"
   },
   "outputs": [],
   "source": [
    "class ChurnitoSystem:\n",
    "    \"\"\"Sistema completo de an√°lisis de churn con chat conversacional\"\"\"\n",
    "    \n",
    "    def __init__(self, churn_model_path=\"./churn_model\"):\n",
    "        print(\"üîÑ Inicializando Churnito...\")\n",
    "        \n",
    "        # Cargar modelo de churn\n",
    "        self.churn_tokenizer = AutoTokenizer.from_pretrained(churn_model_path)\n",
    "        self.churn_model = AutoModelForSequenceClassification.from_pretrained(churn_model_path)\n",
    "        self.churn_model.eval()\n",
    "        \n",
    "        # Cargar artefactos\n",
    "        with open(f\"{churn_model_path}/preprocessing_artifacts.pkl\", \"rb\") as f:\n",
    "            artifacts = pickle.load(f)\n",
    "            self.scaler = artifacts['scaler']\n",
    "            self.label_encoders = artifacts['label_encoders']\n",
    "            self.feature_names = artifacts['feature_names']\n",
    "        \n",
    "        # LLM (ya cargado globalmente)\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "        self.llm_model = llm_model\n",
    "        \n",
    "        # Base de datos de clientes\n",
    "        self.customer_database = df\n",
    "        \n",
    "        print(\"‚úÖ Churnito inicializado\")\n",
    "    \n",
    "    def predict_single_customer(self, customer_data):\n",
    "        \"\"\"Predice churn para un cliente\"\"\"\n",
    "        # Codificar categ√≥ricas\n",
    "        processed = customer_data.copy()\n",
    "        for col_name, encoder in self.label_encoders.items():\n",
    "            if col_name in processed:\n",
    "                try:\n",
    "                    processed[col_name] = encoder.transform([processed[col_name]])[0]\n",
    "                except:\n",
    "                    processed[col_name] = 0\n",
    "        \n",
    "        # Preparar features\n",
    "        features = [float(processed.get(f, 0)) for f in self.feature_names]\n",
    "        features_scaled = self.scaler.transform([features])\n",
    "        \n",
    "        # Crear texto\n",
    "        text = \"Cliente: \" + \" \".join([f\"{n}={v:.2f}\" for n, v in zip(self.feature_names, features_scaled[0])])\n",
    "        \n",
    "        # Predecir\n",
    "        inputs = self.churn_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.churn_model(**inputs)\n",
    "            probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "            churn_prob = probabilities[0][1].item()\n",
    "        \n",
    "        return churn_prob\n",
    "    \n",
    "    def get_at_risk_customers(self, limit=10):\n",
    "        \"\"\"Obtiene clientes en riesgo\"\"\"\n",
    "        df_sample = self.customer_database.sample(n=min(100, len(self.customer_database)), random_state=42)\n",
    "        \n",
    "        at_risk = []\n",
    "        for idx, row in df_sample.iterrows():\n",
    "            customer_data = {\n",
    "                'CreditScore': row.get('CreditScore', 0),\n",
    "                'Geography': row.get('Geography', ''),\n",
    "                'Gender': row.get('Gender', ''),\n",
    "                'Age': row.get('Age', 0),\n",
    "                'Tenure': row.get('Tenure', 0),\n",
    "                'Balance': row.get('Balance', 0),\n",
    "                'NumOfProducts': row.get('NumOfProducts', 0),\n",
    "                'HasCrCard': row.get('HasCrCard', 0),\n",
    "                'IsActiveMember': row.get('IsActiveMember', 0),\n",
    "                'EstimatedSalary': row.get('EstimatedSalary', 0)\n",
    "            }\n",
    "            \n",
    "            churn_prob = self.predict_single_customer(customer_data)\n",
    "            \n",
    "            if churn_prob > 0.5:\n",
    "                at_risk.append({\n",
    "                    'customer_id': int(row.get('CustomerId', idx)),\n",
    "                    'churn_probability': churn_prob,\n",
    "                    'balance': customer_data['Balance'],\n",
    "                    'age': customer_data['Age'],\n",
    "                    'is_active': bool(customer_data['IsActiveMember'])\n",
    "                })\n",
    "        \n",
    "        at_risk.sort(key=lambda x: x['churn_probability'], reverse=True)\n",
    "        return at_risk[:limit]\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Obtiene estad√≠sticas del dataset\"\"\"\n",
    "        return {\n",
    "            'total_customers': len(self.customer_database),\n",
    "            'churned_customers': int(self.customer_database['Exited'].sum()),\n",
    "            'churn_rate': float(self.customer_database['Exited'].mean()),\n",
    "            'avg_balance': float(self.customer_database['Balance'].mean()),\n",
    "            'avg_age': float(self.customer_database['Age'].mean())\n",
    "        }\n",
    "    \n",
    "    def generate_structured_response(self, query, context):\n",
    "        \"\"\"Genera respuesta estructurada con datos reales\"\"\"\n",
    "        response = []\n",
    "        \n",
    "        if \"at_risk_customers\" in context:\n",
    "            at_risk = context[\"at_risk_customers\"]\n",
    "            if at_risk:\n",
    "                high_value_count = sum(1 for c in at_risk if c['balance'] > 100000)\n",
    "                inactive_count = sum(1 for c in at_risk if not c['is_active'])\n",
    "                \n",
    "                response.append(f\"üéØ **An√°lisis de Clientes en Riesgo:**\")\n",
    "                response.append(f\"   ‚Ä¢ {len(at_risk)} clientes identificados con alta probabilidad de churn\")\n",
    "                response.append(f\"   ‚Ä¢ {high_value_count} son clientes de alto valor (Balance > $100k)\")\n",
    "                response.append(f\"   ‚Ä¢ {inactive_count} clientes est√°n inactivos\")\n",
    "                \n",
    "                response.append(\"\\nüìä **Top 5 Clientes Prioritarios:**\")\n",
    "                for i, customer in enumerate(at_risk[:5], 1):\n",
    "                    prob_pct = customer['churn_probability'] * 100\n",
    "                    response.append(\n",
    "                        f\"   {i}. Cliente #{customer['customer_id']}: {prob_pct:.1f}% riesgo, \"\n",
    "                        f\"${customer['balance']:,.0f} balance\\n\"\n",
    "                        f\"      ‚Üí {'üî¥ INACTIVO' if not customer['is_active'] else 'üü° Activo'}\"\n",
    "                    )\n",
    "        \n",
    "        elif \"statistics\" in context:\n",
    "            stats = context[\"statistics\"]\n",
    "            churn_rate = stats['churn_rate'] * 100\n",
    "            \n",
    "            response.append(\"üìä **Estad√≠sticas Actuales:**\")\n",
    "            response.append(f\"   ‚Ä¢ Total de clientes: {stats['total_customers']:,}\")\n",
    "            response.append(f\"   ‚Ä¢ Tasa de churn: {churn_rate:.2f}%\")\n",
    "            response.append(f\"   ‚Ä¢ Balance promedio: ${stats['avg_balance']:,.2f}\")\n",
    "            response.append(f\"   ‚Ä¢ Edad promedio: {stats['avg_age']:.1f} a√±os\")\n",
    "        \n",
    "        return \"\\n\".join(response) if response else \"Churnito a tu servicio. ¬øEn qu√© puedo ayudarte?\"\n",
    "    \n",
    "    def chat(self, query):\n",
    "        \"\"\"Procesa una consulta de chat\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        context = {}\n",
    "        \n",
    "        # Detectar intenciones\n",
    "        if any(word in query_lower for word in [\"riesgo\", \"top\", \"clientes\", \"fuga\", \"muestra\"]):\n",
    "            context[\"at_risk_customers\"] = self.get_at_risk_customers(limit=10)\n",
    "        \n",
    "        if any(word in query_lower for word in [\"estad√≠stica\", \"tasa\", \"cu√°ntos\", \"total\"]):\n",
    "            context[\"statistics\"] = self.get_statistics()\n",
    "        \n",
    "        # Generar respuesta\n",
    "        return self.generate_structured_response(query, context)\n",
    "\n",
    "# Inicializar Churnito\n",
    "churnito = ChurnitoSystem()\n",
    "print(\"\\nü§ñ ¬°Churnito est√° listo para conversar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section9"
   },
   "source": [
    "## 9Ô∏è‚É£ Chat Interactivo con Churnito\n",
    "\n",
    "¬°Haz preguntas a Churnito sobre tus clientes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chat_interface"
   },
   "outputs": [],
   "source": [
    "def chat_with_churnito():\n",
    "    \"\"\"Interfaz de chat simple\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"üí¨ CHAT CON CHURNITO\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nEjemplos de preguntas:\")\n",
    "    print(\"  ‚Ä¢ Mu√©strame los 10 clientes con mayor riesgo de fuga\")\n",
    "    print(\"  ‚Ä¢ ¬øCu√°l es la tasa de churn actual?\")\n",
    "    print(\"  ‚Ä¢ Dame estad√≠sticas generales\")\n",
    "    print(\"\\nEscribe 'salir' para terminar\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nüßë T√∫: \")\n",
    "        \n",
    "        if query.lower() in ['salir', 'exit', 'quit']:\n",
    "            print(\"\\nüëã ¬°Hasta luego!\")\n",
    "            break\n",
    "        \n",
    "        if not query.strip():\n",
    "            continue\n",
    "        \n",
    "        response = churnito.chat(query)\n",
    "        print(f\"\\nü§ñ Churnito:\\n{response}\")\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Iniciar chat\n",
    "chat_with_churnito()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section10"
   },
   "source": [
    "## üîü Consultas R√°pidas (sin interfaz interactiva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick_queries"
   },
   "outputs": [],
   "source": [
    "# Ejemplo 1: Clientes en riesgo\n",
    "print(\"üìä CONSULTA: Clientes en riesgo\\n\")\n",
    "response = churnito.chat(\"Mu√©strame los 10 clientes con mayor riesgo de fuga\")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Ejemplo 2: Estad√≠sticas\n",
    "print(\"üìä CONSULTA: Estad√≠sticas generales\\n\")\n",
    "response = churnito.chat(\"Dame las estad√≠sticas de churn\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section11"
   },
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Predicci√≥n para Cliente Espec√≠fico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "single_prediction"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de cliente\n",
    "ejemplo_cliente = {\n",
    "    'CreditScore': 650,\n",
    "    'Geography': 'France',\n",
    "    'Gender': 'Female',\n",
    "    'Age': 42,\n",
    "    'Tenure': 2,\n",
    "    'Balance': 125000,\n",
    "    'NumOfProducts': 1,\n",
    "    'HasCrCard': 1,\n",
    "    'IsActiveMember': 0,\n",
    "    'EstimatedSalary': 75000\n",
    "}\n",
    "\n",
    "churn_prob = churnito.predict_single_customer(ejemplo_cliente)\n",
    "\n",
    "print(\"üîç PREDICCI√ìN PARA CLIENTE ESPEC√çFICO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Datos del cliente:\")\n",
    "for key, value in ejemplo_cliente.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüéØ Probabilidad de churn: {churn_prob*100:.2f}%\")\n",
    "print(f\"üö¶ Nivel de riesgo: {'üî¥ ALTO' if churn_prob > 0.7 else 'üü° MEDIO' if churn_prob > 0.5 else 'üü¢ BAJO'}\")\n",
    "\n",
    "if churn_prob > 0.7:\n",
    "    print(\"\\n‚ö†Ô∏è  RECOMENDACI√ìN: Contactar inmediatamente para retenci√≥n\")\n",
    "elif churn_prob > 0.5:\n",
    "    print(\"\\nüí° RECOMENDACI√ìN: Implementar programa de retenci√≥n preventivo\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ RECOMENDACI√ìN: Monitoreo rutinario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section12"
   },
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Descargar Modelos (Opcional)\n",
    "\n",
    "Si quieres descargar los modelos entrenados para usarlos localmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_models"
   },
   "outputs": [],
   "source": [
    "# Comprimir modelo de churn\n",
    "!zip -r churn_model.zip churn_model/\n",
    "\n",
    "print(\"\\n‚úÖ Modelo comprimido: churn_model.zip\")\n",
    "print(\"üì• Desc√°rgalo desde el panel de archivos (üìÅ) en el lado izquierdo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "## üéâ ¬°Felicidades!\n",
    "\n",
    "Has completado la configuraci√≥n de Churnito en Google Colab.\n",
    "\n",
    "### üìö Recursos adicionales:\n",
    "- [Repositorio GitHub](https://github.com/CuchoLeo/Fuga)\n",
    "- [Documentaci√≥n completa](https://github.com/CuchoLeo/Fuga/blob/main/README.md)\n",
    "\n",
    "### ü§ù Creado por:\n",
    "**Churnito Team** - Sistema de predicci√≥n de churn con IA\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
