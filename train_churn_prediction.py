import torch
import numpy as np
import pandas as pd
from pathlib import Path
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
from transformers import Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import os
import shutil

# Set environment variables
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

print("="*70)
print("SISTEMA DE PREDICCI√ìN DE CHURN - CLIENTES ALTO VALOR")
print("="*70)

# ============================================================================
# 1. CARGA Y PREPROCESAMIENTO DEL DATASET
# ============================================================================

def load_and_preprocess_data(csv_path):
    """
    Carga el dataset de Kaggle: Bank Customer Churn
    Esperado: Churn_Modelling.csv
    """
    print(f"\nüìä Cargando dataset desde: {csv_path}")
    
    # Leer CSV
    df = pd.read_csv(csv_path)
    print(f"‚úì Dataset cargado: {len(df)} registros, {len(df.columns)} columnas")
    
    # Mostrar informaci√≥n del dataset
    print(f"\nColumnas disponibles: {list(df.columns)}")
    print(f"\nDistribuci√≥n de Churn:")
    print(df['Exited'].value_counts())
    print(f"Tasa de Churn: {df['Exited'].mean()*100:.2f}%")
    
    # Filtrar clientes de alto valor (>$100,000 en Balance)
    if 'Balance' in df.columns:
        high_value_mask = df['Balance'] > 100000
        print(f"\nüí∞ Clientes alto valor (Balance > $100k): {high_value_mask.sum()} ({high_value_mask.mean()*100:.1f}%)")
        print(f"Churn rate alto valor: {df[high_value_mask]['Exited'].mean()*100:.2f}%")
    
    # Eliminar columnas no relevantes
    cols_to_drop = ['RowNumber', 'CustomerId', 'Surname']
    df = df.drop([col for col in cols_to_drop if col in df.columns], axis=1)
    
    # Codificar variables categ√≥ricas
    label_encoders = {}
    categorical_cols = ['Geography', 'Gender']
    
    for col in categorical_cols:
        if col in df.columns:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
            label_encoders[col] = le
    
    # Separar features y target
    X = df.drop('Exited', axis=1)
    y = df['Exited']
    
    # Normalizar features num√©ricas
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Convertir a texto para el modelo de lenguaje
    # Creamos descripciones textuales de cada cliente
    feature_names = X.columns.tolist()
    
    print(f"\n‚úì Preprocesamiento completado")
    print(f"Features: {len(feature_names)}")
    
    return X_scaled, y.values, feature_names, scaler, label_encoders

def create_text_from_features(X, feature_names, y=None):
    """
    Convierte features num√©ricas en descripciones textuales
    para entrenar el modelo de lenguaje
    """
    texts = []
    labels = []
    
    for idx in range(len(X)):
        # Crear descripci√≥n del cliente
        feature_values = X[idx]
        text_parts = ["Cliente:"]
        
        for name, value in zip(feature_names, feature_values):
            text_parts.append(f"{name}={value:.2f}")
        
        text = " ".join(text_parts)
        
        # Si tenemos labels, agregar la predicci√≥n esperada
        if y is not None:
            label_text = "CHURN" if y[idx] == 1 else "RETIENE"
            text += f" -> Predicci√≥n: {label_text}"
            labels.append(y[idx])
        
        texts.append(text)
    
    return texts, labels if y is not None else texts

# ============================================================================
# 2. CONFIGURACI√ìN DEL MODELO
# ============================================================================

# Ruta del dataset (ajustar seg√∫n tu ubicaci√≥n)
DATA_PATH = "Churn_Modelling.csv"

# Verificar si existe el archivo
if not Path(DATA_PATH).exists():
    print(f"\n‚ö†Ô∏è  ADVERTENCIA: No se encontr√≥ el archivo {DATA_PATH}")
    print("\nPara descargar el dataset:")
    print("1. Ve a: https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling")
    print("2. Descarga 'Churn_Modelling.csv'")
    print("3. Col√≥calo en el mismo directorio que este script")
    print("\nO usa Kaggle API:")
    print("   kaggle datasets download -d shrutimechlearn/churn-modelling")
    exit(1)

# Cargar y procesar datos
X, y, feature_names, scaler, label_encoders = load_and_preprocess_data(DATA_PATH)

# Dividir en train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nüìö Divisi√≥n de datos:")
print(f"Training: {len(X_train)} muestras")
print(f"Testing: {len(X_test)} muestras")

# Convertir a texto
train_texts, train_labels = create_text_from_features(X_train, feature_names, y_train)
test_texts, test_labels = create_text_from_features(X_test, feature_names, y_test)

# Load the model and tokenizer para clasificaci√≥n
model_id = "distilbert-base-uncased"  # Modelo m√°s ligero para clasificaci√≥n
print(f"\nü§ñ Cargando modelo base: {model_id}")
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSequenceClassification.from_pretrained(
    model_id, 
    num_labels=2,  # Binario: Churn / No Churn
    torch_dtype=torch.float32
)

# Set the padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ============================================================================
# 3. PREPARACI√ìN DEL DATASET
# ============================================================================

max_length = 256  # Aumentado para acomodar features

print(f"\n‚öôÔ∏è  Tokenizando {len(train_texts)} ejemplos de entrenamiento...")
train_encodings = tokenizer(
    train_texts,
    padding="max_length",
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

print(f"‚öôÔ∏è  Tokenizando {len(test_texts)} ejemplos de prueba...")
test_encodings = tokenizer(
    test_texts,
    padding="max_length",
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

class ChurnDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {
            "input_ids": self.encodings["input_ids"][idx],
            "attention_mask": self.encodings["attention_mask"][idx],
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }
        return item

train_dataset = ChurnDataset(train_encodings, train_labels)
test_dataset = ChurnDataset(test_encodings, test_labels)

# ============================================================================
# 4. CONFIGURACI√ìN DE ENTRENAMIENTO
# ============================================================================

output_dir = Path("churn_model").resolve()
checkpoint_dir = Path("checkpoint_churn").resolve()

# Limpiar archivos antiguos
if checkpoint_dir.exists():
    shutil.rmtree(checkpoint_dir)

print(f"\nüìÅ Directorio de salida: {output_dir}")
print(f"üñ•Ô∏è  CUDA disponible: {torch.cuda.is_available()}")

# Training arguments optimizados para churn prediction
training_args = TrainingArguments(
    output_dir=str(checkpoint_dir),
    num_train_epochs=1,  # Reducido a 1 √©poca para entrenamiento r√°pido
    per_device_train_batch_size=32,  # Aumentado para procesar m√°s r√°pido
    per_device_eval_batch_size=32,  # Aumentado para procesar m√°s r√°pido
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=50,  # Menos logging = m√°s r√°pido
    use_cpu=True,
    dataloader_pin_memory=False,
    dataloader_num_workers=0,
    remove_unused_columns=False,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
)

print("\n" + "="*70)
print("üöÄ INICIANDO ENTRENAMIENTO DEL MODELO DE PREDICCI√ìN DE CHURN")
print("="*70)

# ============================================================================
# CALCULAR CLASS WEIGHTS PARA MANEJAR DESBALANCE
# ============================================================================

# Contar distribuci√≥n de clases
from sklearn.utils.class_weight import compute_class_weight

classes = np.unique(y_train)
class_weights_array = compute_class_weight(
    class_weight='balanced',
    classes=classes,
    y=y_train
)
class_weights = torch.tensor(class_weights_array, dtype=torch.float32)

print(f"\n‚öñÔ∏è  Balanceo de clases:")
print(f"Clase 0 (NO CHURN): {(y_train == 0).sum()} muestras, weight={class_weights[0]:.3f}")
print(f"Clase 1 (CHURN):    {(y_train == 1).sum()} muestras, weight={class_weights[1]:.3f}")
print(f"Ratio: {class_weights[1]/class_weights[0]:.2f}x m√°s peso para clase minoritaria")

# ============================================================================
# CUSTOM TRAINER CON CLASS WEIGHTS
# ============================================================================

class WeightedTrainer(Trainer):
    """Trainer personalizado que usa class weights en la funci√≥n de p√©rdida"""

    def __init__(self, *args, class_weights=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights

    def compute_loss(self, model, inputs, return_outputs=False):
        # Extraer labels sin modificar el dict original
        labels = inputs.get("labels")

        # Forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")

        # Calcular p√©rdida con class weights
        if self.class_weights is not None:
            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)
            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        else:
            # Fallback a p√©rdida por defecto
            loss = outputs.get("loss")

        return (loss, outputs) if return_outputs else loss

# Funci√≥n para calcular m√©tricas
def compute_metrics(eval_pred):
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)

    # Calcular m√©tricas con zero_division=0 para evitar warnings
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='binary', zero_division=0
    )

    # Matriz de confusi√≥n
    cm = confusion_matrix(labels, predictions)
    tn, fp, fn, tp = cm.ravel()

    print(f"\nüìä Matriz de Confusi√≥n:")
    print(f"   TN={tn}, FP={fp}")
    print(f"   FN={fn}, TP={tp}")

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
    }

trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
    class_weights=class_weights,  # Pasar class weights al trainer
)

# ============================================================================
# 5. ENTRENAMIENTO
# ============================================================================

try:
    train_result = trainer.train()
    print("\n‚úÖ Entrenamiento completado!")

    # Evaluar en test set
    print("\nüìä Evaluando modelo en conjunto de prueba...")
    eval_results = trainer.evaluate()

    print("\n" + "="*70)
    print("‚úÖ ENTRENAMIENTO COMPLETADO")
    print("="*70)
    print(f"üìä Accuracy:  {eval_results.get('eval_accuracy', 0):.4f}")
    print(f"üìä Precision: {eval_results.get('eval_precision', 0):.4f}")
    print(f"üìä Recall:    {eval_results.get('eval_recall', 0):.4f}")
    print(f"üìä F1-Score:  {eval_results.get('eval_f1', 0):.4f}")

    # Explicaci√≥n de m√©tricas
    print("\nüí° Interpretaci√≥n:")
    print("   - Accuracy:  % de predicciones correctas (total)")
    print("   - Precision: De los que predecimos CHURN, % que realmente hacen churn")
    print("   - Recall:    De los que hacen CHURN, % que detectamos correctamente")
    print("   - F1-Score:  Balance entre Precision y Recall")

    # Advertencias
    if eval_results.get('eval_precision', 0) < 0.5:
        print("\n‚ö†Ô∏è  ADVERTENCIA: Precision baja. El modelo predice muchos falsos positivos.")
    if eval_results.get('eval_recall', 0) < 0.5:
        print("\n‚ö†Ô∏è  ADVERTENCIA: Recall bajo. El modelo no detecta suficientes churners.")
    if eval_results.get('eval_f1', 0) > 0.7:
        print("\n‚úÖ F1-Score bueno (>0.7). Modelo balanceado entre precision y recall.")

except Exception as e:
    print(f"\n‚ùå Error durante el entrenamiento: {e}")
    import traceback
    traceback.print_exc()

# ============================================================================
# 6. GUARDAR MODELO
# ============================================================================

print("\n" + "="*70)
print("üíæ GUARDANDO MODELO Y ARTEFACTOS")
print("="*70)

try:
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Guardar modelo
    print(f"\nüì¶ Guardando modelo en: {output_dir}")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # Guardar scaler y encoders
    import pickle
    
    artifacts = {
        'scaler': scaler,
        'label_encoders': label_encoders,
        'feature_names': feature_names
    }
    
    with open(output_dir / 'preprocessing_artifacts.pkl', 'wb') as f:
        pickle.dump(artifacts, f)
    
    print("‚úÖ Artefactos de preprocesamiento guardados")
    
    # Verificar archivos
    print("\n" + "="*70)
    print("ARCHIVOS GUARDADOS:")
    print("="*70)
    for f in sorted(output_dir.iterdir()):
        size = f.stat().st_size / (1024*1024)
        print(f"‚úì {f.name}: {size:.2f} MB")
    
    print("\n" + "="*70)
    print("‚úÖ MODELO DE PREDICCI√ìN DE CHURN ENTRENADO Y GUARDADO")
    print("="*70)
    print(f"\nüí° Para usar el modelo:")
    print(f"   1. Cargar desde: {output_dir}")
    print(f"   2. Aplicar a clientes de alto valor (Balance > $100k)")
    print(f"   3. Priorizar retenci√≥n de clientes con alta probabilidad de churn")
    print(f"\nüí∞ ROI Estimado:")
    print(f"   Costo retenci√≥n = 1/5 del costo de adquisici√≥n")
    print(f"   Clientes salvados/mes: reducci√≥n de 2,500 fugas actuales")
    
except Exception as e:
    print(f"\n‚ùå Error guardando modelo: {e}")
    import traceback
    traceback.print_exc()

# Limpiar checkpoint directory
if checkpoint_dir.exists():
    shutil.rmtree(checkpoint_dir)
    print("\nüßπ Archivos temporales eliminados")

print("\n" + "="*70)
print("üéØ PR√ìXIMOS PASOS RECOMENDADOS:")
print("="*70)
print("1. Validar predicciones en clientes de alto valor")
print("2. Implementar sistema de alertas tempranas")
print("3. Integrar con CRM para acciones de retenci√≥n")
print("4. Monitorear m√©tricas: Recall (capturar churners) y Precision (evitar falsos positivos)")
print("="*70)